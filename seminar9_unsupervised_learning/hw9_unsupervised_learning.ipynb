{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKoj5pR-iP0"
      },
      "source": [
        "# Домашняя работа к девятому семинару"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm6HUSvX-sRt"
      },
      "source": [
        "Использование алгоритмов понижения размерности для улучшения классификации новостей (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)\n",
        "\n",
        "Цель задания: Исследовать влияние различных методов понижения размерности на качество классификации текстовых данных.\n",
        "\n",
        "Датасет: Набор данных новостных статей\n",
        "(датасет '20 Newsgroups' доступный в sklearn.datasets).\n",
        "\n",
        "Задачи:\n",
        "\n",
        "1. Загрузите датасет '20 Newsgroups' из sklearn.\n",
        "\n",
        "2. Проведите предобработку данных (очистка текста, удаление стоп-слов, векторизация с использованием TF-IDF).\n",
        "\n",
        "3. Примените к полученным векторам TF-IDF следующие методы понижения размерности:\n",
        " - PCA (Principal Component Analysis)\n",
        " - t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        " - UMAP (Uniform Manifold Approximation and Projection).\n",
        "\n",
        "4. После понижения размерности данных используйте любой метод машинного обучения для классификации новостей по темам.\n",
        "\n",
        "5. Сравните качество классификации для каждого метода понижения размерности. Используйте метрики точности и F1-меру.\n",
        "\n",
        "6. Визуализируйте двумерное представление данных для каждого метода понижения размерности, чтобы оценить, как алгоритмы справляются с сепарацией классов.\n",
        "\n",
        "7. Напишите отчёт, в котором обсудите, какой метод понижения размерности оказал наиболее значительное влияние на качество классификации и почему."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a01QGy6zHq99"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA, TruncatedSVD, IncrementalPCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import umap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfvVo0m_H_LE"
      },
      "source": [
        "## 1. Загрузка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IbmsNtaKH0Zc"
      },
      "outputs": [],
      "source": [
        "newsgroups = fetch_20newsgroups(subset='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5whNvjjH4Dk"
      },
      "source": [
        "## 2. Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yb2k_2QnH7o1"
      },
      "outputs": [],
      "source": [
        "# Очистка текста и удаление стоп-слов\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4MihNY4a4xlJ"
      },
      "outputs": [],
      "source": [
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBsLmSN45mp"
      },
      "source": [
        "## 3. Применение методов понижения размерности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zhAu2e9H5D7x"
      },
      "outputs": [],
      "source": [
        "# # PCA\n",
        "# pca = PCA(n_components=2)\n",
        "# X_train_pca = pca.fit_transform(X_train.toarray())\n",
        "# X_test_pca = pca.transform(X_test.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При попытке преобразовать разрежённую матрицу в плотную с помощью метода toarray() возникла ошибка `MemoryError`. Ошибка `MemoryError` возникает, когда системе не хватает памяти для выполнения операции. В данном случае, проблема связана с попыткой преобразовать разреженную матрицу в плотную, что требует значительного объема памяти.\n",
        "\n",
        "Попробуем решить эту проблему за счёт использования разреженных матриц: Вместо преобразования разреженной матрицы в плотную, используем методы, которые работают непосредственно с разреженными матрицами. Например, можно использовать TruncatedSVD из библиотеки scikit-learn, который работает с разреженными матрицами:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=2)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Предположим, что X_train - это разреженная матрица\n",
        "batch_size = 1000  # Размер пакета\n",
        "n_components = 2  # Количество компонент PCA\n",
        "\n",
        "ipca = IncrementalPCA(n_components=n_components)\n",
        "\n",
        "# Обработка данных пакетами\n",
        "for i in range(0, X_train.shape[0], batch_size):\n",
        "    X_batch = X_train[i:i + batch_size].toarray()\n",
        "    ipca.partial_fit(X_batch)\n",
        "\n",
        "# Преобразование данных\n",
        "X_train_pca = np.vstack([ipca.transform(X_train[i:i + batch_size].toarray()) for i in range(0, X_train.shape[0], batch_size)])\n",
        "X_test_pca = np.vstack([ipca.transform(X_test[i:i + batch_size].toarray()) for i in range(0, X_test.shape[0], batch_size)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training PCA: 100%|██████████| 16/16 [22:48<00:00, 85.50s/it]\n",
            "Transforming Train Data: 100%|██████████| 16/16 [00:15<00:00,  1.04it/s]\n",
            "Transforming Test Data: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\n"
          ]
        }
      ],
      "source": [
        "# Предположим, что X_train - это разреженная матрица\n",
        "batch_size = 1000  # Размер пакета\n",
        "n_components = 2  # Количество компонент PCA\n",
        "\n",
        "ipca = IncrementalPCA(n_components=n_components)\n",
        "\n",
        "# Обработка данных пакетами с прогрессбаром\n",
        "for i in tqdm(range(0, X_train.shape[0], batch_size), desc=\"Training PCA\"):\n",
        "    X_batch = X_train[i:i + batch_size].toarray()\n",
        "    ipca.partial_fit(X_batch)\n",
        "\n",
        "# Преобразование данных с прогрессбаром\n",
        "X_train_pca = np.vstack([ipca.transform(X_train[i:i + batch_size].toarray()) for i in tqdm(range(0, X_train.shape[0], batch_size), desc=\"Transforming Train Data\")])\n",
        "X_test_pca = np.vstack([ipca.transform(X_test[i:i + batch_size].toarray()) for i in tqdm(range(0, X_test.shape[0], batch_size), desc=\"Transforming Test Data\")])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "4f-uhGJ25L_5",
        "outputId": "b4d8219d-b083-47d2-8374-a50e854ff023"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 19.5 GiB for an array with shape (15076, 173451) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# t-SNE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m X_train_tsne \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m X_test_tsne \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mtoarray())\n",
            "File \u001b[1;32me:\\repo\\python_libraries_DS\\venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32me:\\repo\\python_libraries_DS\\venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.5 GiB for an array with shape (15076, 173451) and data type float64"
          ]
        }
      ],
      "source": [
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_train_tsne = tsne.fit_transform(X_train.toarray())\n",
        "X_test_tsne = tsne.transform(X_test.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J6DZneYY5Ovp"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 19.5 GiB for an array with shape (15076, 173451) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# UMAP\u001b[39;00m\n\u001b[0;32m      2\u001b[0m umap_reducer \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m X_train_umap \u001b[38;5;241m=\u001b[39m umap_reducer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m X_test_umap \u001b[38;5;241m=\u001b[39m umap_reducer\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mtoarray())\n",
            "File \u001b[1;32me:\\repo\\python_libraries_DS\\venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32me:\\repo\\python_libraries_DS\\venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.5 GiB for an array with shape (15076, 173451) and data type float64"
          ]
        }
      ],
      "source": [
        "# UMAP\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "X_train_umap = umap_reducer.fit_transform(X_train.toarray())\n",
        "X_test_umap = umap_reducer.transform(X_test.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BsYTS_H5RMB"
      },
      "source": [
        "## 4. Классификация новостей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HORL0XH5XtZ"
      },
      "source": [
        "Для классификации можно использовать, например, логистическую регрессию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDZo-Uep5k-x"
      },
      "outputs": [],
      "source": [
        "# Функция для обучения и оценки модели\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmpW8D0k5mDp"
      },
      "outputs": [],
      "source": [
        "# Оценка моделей\n",
        "accuracy_pca, f1_pca = evaluate_model(X_train_pca, X_test_pca, y_train, y_test)\n",
        "accuracy_tsne, f1_tsne = evaluate_model(X_train_tsne, X_test_tsne, y_train, y_test)\n",
        "accuracy_umap, f1_umap = evaluate_model(X_train_umap, X_test_umap, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpfXnQTw5qdi"
      },
      "source": [
        "## 5. Сравнение качества классификации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PMKFMgt5u3J"
      },
      "outputs": [],
      "source": [
        "print(f\"PCA: Accuracy = {accuracy_pca}, F1 Score = {f1_pca}\")\n",
        "print(f\"t-SNE: Accuracy = {accuracy_tsne}, F1 Score = {f1_tsne}\")\n",
        "print(f\"UMAP: Accuracy = {accuracy_umap}, F1 Score = {f1_umap}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4SZCR_b511J"
      },
      "source": [
        "## 6. Визуализация данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gtFBcLo56pT"
      },
      "outputs": [],
      "source": [
        "def plot_2d(X, y, title):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=5)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN3e9HJj6Dd5"
      },
      "outputs": [],
      "source": [
        "plot_2d(X_train_pca, y_train, 'PCA')\n",
        "plot_2d(X_train_tsne, y_train, 't-SNE')\n",
        "plot_2d(X_train_umap, y_train, 'UMAP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcCqkET6JtB"
      },
      "source": [
        "## 7. Написание отчета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3up5pxNz6OHh"
      },
      "source": [
        "В отчете обсудите результаты, сравните метрики точности и F1-меры для каждого метода понижения размерности, а также визуализации. Обратите внимание на то, какой метод лучше справляется с сепарацией классов и почему."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
